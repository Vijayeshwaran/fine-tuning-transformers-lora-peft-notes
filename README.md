# Fine-Tuning Transformers using LoRA & PEFT

This repository contains my notes and hands-on experiments on using Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning of transformer models, using the Hugging Face `peft` library.

## ðŸ§  What I Learned

- Basics of model fine-tuning
- What LoRA is and how it reduces training cost
- How to integrate LoRA with Hugging Face's PEFT
- Tried LoRA on vision models like ViT and ResNet

## ðŸ”§ Tools & Frameworks

- Python
- PyTorch
- Hugging Face Transformers & PEFT
- Google Colab

## ðŸ“’ Notebook

All hands-on work is in `notebooks/lora_peft_finetuning_notes.ipynb`

## ðŸ§© Future Work

- Benchmark with full fine-tuning
- Apply to a mini-project
